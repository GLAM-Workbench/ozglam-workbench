{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harvesting data from Home\n",
    "\n",
    "This is an example of how my original recipe for [harvesting data from The Bulletin](Harvesting-data-from-the-Bulletin.ipynb) can be modified for other journals.\n",
    "\n",
    "If you'd like a pre-harvested dataset of all the Home covers (229 images in a 3.3gb zip file), open this link using your preferred BitTorrent client: <magnet:?xt=urn:btih:7888BCEA44E5FF5670931A3394369E5018BFC32B&dn=home-quarterly.zip>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the libraries we need.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for this journal\n",
    "# Edit as necessary for a new journal\n",
    "data_dir = '../../data/Trove/Home'\n",
    "os.makedirs(data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the issue data\n",
    "\n",
    "Each issue of a digitised journal like has it's own unique identifier. You've probably noticed them in the urls of Trove resources. They look something like this `nla.obj-362409353`. Once we have the identifier for an issue we can easily download the contents, but how do we get a complete list of identifiers?\n",
    "\n",
    "The [harvesting data from the Bulletin](Harvesting-data-from-the-Bulletin.ipynb) notebook explains how we can find a url that lists all the available issues of a journal.\n",
    "\n",
    "This is the url we need to start harvesting issue metadata about *Home*. You could easily modify this to get metadata from another journal by changing the identifier.\n",
    "\n",
    "```\n",
    "https://nla.gov.au/nla.obj-362409353/browse?startIdx=0&rows=20&op=c\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just the url we found above, with a slot into which we can insert the startIdx value\n",
    "# If you want to download data from another journal, just change the nla.obj identifier to point to the journal.\n",
    "start_url = 'https://nla.gov.au/nla.obj-362409353/browse?startIdx={}&rows=20&op=c'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The initial startIdx value\n",
    "start = 0\n",
    "# Number of results per page\n",
    "n = 20\n",
    "issues = []\n",
    "# If there aren't 20 results on the page then we've reached the end, so continue harvesting until that happens.\n",
    "while n == 20:\n",
    "    # Get the browse page\n",
    "    response = requests.get(start_url.format(start))\n",
    "    # Beautifulsoup turns the HTML into an easily navigable structure\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    # Find all the divs containing issue details and loop through them\n",
    "    details = soup.find_all(class_='l-item-info')\n",
    "    for detail in details:\n",
    "        issue = {}\n",
    "        # Get the issue id\n",
    "        issue['id'] = detail.dt.a.string\n",
    "        rows = detail.find_all('dd')\n",
    "        # Get the issue details\n",
    "        issue['details'] = rows[2].p.string\n",
    "        # Get the number of pages\n",
    "        issue['pages'] = re.search(r'^(\\d+)', detail.find('a', class_=\"browse-child\").text, flags=re.MULTILINE).group(1)\n",
    "        issues.append(issue)\n",
    "        print(issue)\n",
    "        time.sleep(0.2)\n",
    "    # Increment the startIdx\n",
    "    start += n\n",
    "    # Set n to the number of results on the current page\n",
    "    n = len(details)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the harvested results as a JSON file in case we need them later on\n",
    "with open('{}/home_issues.json'.format(data_dir), 'w') as outfile:\n",
    "    json.dump(issues, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the saved JSON file\n",
    "with open('{}/home_issues.json'.format(data_dir), 'r') as infile:\n",
    "    issues = json.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up the metadata\n",
    "\n",
    "So far we've just grabbed the complete issue details as a single string. It would be good to parse this string so that we have the dates, volume and issue numbers in separate fields. As is always the case, there's a bit of variation in the way this information is recorded. The code below tries out different combinations and then saves the structured data in a Python list.\n",
    "\n",
    "I had to modify the code I used with the *Bulletin* due to slight variations in the way the issue data was recorded. For example, issue dates for *Home* use the full names of months, while the *Bulletin* records used abbreviations. It's likely that there will be other variations between journals, so you might have to adjust this code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import arrow\n",
    "from arrow.parser import ParserError\n",
    "issues_data = []\n",
    "# Loop through the issues\n",
    "for issue in issues:\n",
    "    issue_data = {}\n",
    "    issue_data['id'] = issue['id']\n",
    "    issue_data['pages'] = int(issue['pages'])\n",
    "    print(issue['details'])\n",
    "    try:\n",
    "        # This pattern looks for details in the form: Vol. 2 No. 3 (2 Jul 1878)\n",
    "        details = re.search(r'(.*)Vol. (\\d+) No\\.* (\\d+) \\((.+)\\)', issue['details'].strip())\n",
    "        issue_data['label'] = details.group(1).strip()\n",
    "        issue_data['volume'] = details.group(2)\n",
    "        issue_data['number'] = details.group(3)\n",
    "        date = details.group(4)\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            # This pattern looks for details in the form: No. 3 (2 Jul 1878)\n",
    "            details = re.search(r'No. (\\d+) \\((.+)\\)', issue['details'].strip())\n",
    "            issue_data['label'] = ''\n",
    "            issue_data['volume'] = ''\n",
    "            issue_data['number'] = details.group(1)\n",
    "            date = details.group(2)\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                # This pattern looks for details in the form: Bulletin Christmas Edition (2 Jul 1878)\n",
    "                details = re.search(r'(.*) \\((.+)\\)', issue['details'].strip())\n",
    "                issue_data['label'] = details.group(1)\n",
    "                issue_data['volume'] = ''\n",
    "                issue_data['number'] = ''\n",
    "                date = details.group(2)\n",
    "            except AttributeError:\n",
    "                # This pattern looks for details in the form: Bulletin 1878 Jul 3\n",
    "                details = re.search(r'Bulletin (.+)', issue['details'].strip())\n",
    "                date_str = details.group(1)\n",
    "                # Date is wrong way round, split and reverse\n",
    "                date = ' '.join(reversed(date_str.split()))\n",
    "                issue_data['label'] = ''\n",
    "                issue_data['volume'] = ''\n",
    "                issue_data['number'] = ''\n",
    "    # Normalise months\n",
    "    date = date.replace('Sept', 'Sep').replace('Sepember', 'September').replace('July August', 'July').replace('September October', 'September').replace('  ', ' ')\n",
    "    # Convert date to ISO format\n",
    "    try:\n",
    "        issue_data['date'] = arrow.get(date, 'D MMMM YYYY').isoformat()[:-15]\n",
    "    except ParserError:\n",
    "        issue_data['date'] = arrow.get(date, 'D MMM YYYY').isoformat()[:-15]\n",
    "    issues_data.append(issue_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save as CSV\n",
    "\n",
    "Now the issues data is in a nice, structured form, we can load it into a Pandas dataframe. This allows us to do things like find the total number of pages digitised.\n",
    "\n",
    "We can also save the metadata as a CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Convert issues metadata into a dataframe\n",
    "df = pd.DataFrame(issues_data, columns=['id', 'label', 'volume', 'number', 'date', 'pages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the total number of pages\n",
    "df['pages'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata as a CSV.\n",
    "df.to_csv('{}/home_issues.csv'.format(data_dir), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download front covers\n",
    "\n",
    "Options for downloading images, PDFs and text are described in the [harvesting data from the Bulletin](Harvesting-data-from-the-Bulletin.ipynb) notebook. In this recipe we'll just download the fromt covers (because they're awesome).\n",
    "\n",
    "The code below checks to see if an image has already been saved before downloading it, so if the process is interrupted you can just run it again to pick up where it stopped. If more issues are added to Trove you could run it again to pick up any new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import io\n",
    "# Prepare a directory to save the images into\n",
    "output_dir = data_dir + '/images'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "# Loop through the issue metadata\n",
    "for issue in issues_data:\n",
    "    print(issue['id'])\n",
    "    id = issue['id']\n",
    "    # Check to see if the first page of this issue has already been downloaded\n",
    "    if not os.path.exists('{}/{}-1.jpg'.format(output_dir, id)):\n",
    "        url = 'https://nla.gov.au/{}/download?downloadOption=zip&firstPage=0&lastPage=0'.format(id)\n",
    "        # Get the file\n",
    "        r = requests.get(url)\n",
    "        # The image is in a zip, so we need to extract the contents into the output directory\n",
    "        z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "        z.extractall(output_dir)\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
