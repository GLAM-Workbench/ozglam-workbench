{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Harvesting data from the Bulletin (or any other digitised journal)\n",
    "\n",
    "The National Library of Australia is digitising lots of interesting and useful journals like *The Bulletin*. These can be downloaded as images, PDFs or as plain text. However, as there's no API access, there's no obvious way of mechanising the download process to create large data sets. But with a little reverse engineering of the interface and some screen scraping it *is* possible. This notebook will show you how, providing all you need to download all the issue metadata of *The Bulletin*, along with high-resoluation images of the covers, and the OCRd text.\n",
    "\n",
    "The code here could be easily modified to download data from another journal.\n",
    "\n",
    "You can download [pre-harvested text and metadata](https://github.com/wragge/ozglam-workbench/blob/master/Trove/Cookbook/Harvesting-data-from-the-Bulletin.ipynb) from the Trove texts repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import the libraries we need.\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure data directory exists\n",
    "data_dir = '../../data/Trove/Bulletin'\n",
    "os.makedirs(data_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the issue data\n",
    "\n",
    "Each issue of a journal like *The Bulletin* has it's own unique identifier. You've probably noticed them in the urls of Trove resources. They look something like this `nla.obj-188537163`. Once we have the identifier for an issue we can easily download the contents, but how do we get a complete list of identifiers?\n",
    "\n",
    "This is where we need to do a bit of reverse engineering. One essential tool when you're doing this sort of work is your browser console. It varies a bit across browsers, but usually you can open the console by right clicking on a page and selecting the 'Inspect' option. Once it's open, choose the 'Network' tab, then go to the [parent page](https://nla.gov.au/nla.obj-68375465/) for *The Bulletin* in the Trove Digital Library. Now click on the 'Browse' option in the Trove menu. Look carefully through all the entries in the Network console, and you should find this link:\n",
    "\n",
    "```\n",
    "https://nla.gov.au/nla.obj-68375465/browse?startIdx=0&rows=20&op=c\n",
    "```\n",
    "\n",
    "This link retrieves the issue details that are then displayed in the browse pane, but it's just a normal url, that delivers normal HTML. [Click here to open it.](https://nla.gov.au/nla.obj-68375465/browse?startIdx=0&rows=20&op=c)\n",
    "\n",
    "As you may have noticed, the url contains a `startIdx` parameter. By increasing this value, you can navigate your way through the complete set of issues.\n",
    "\n",
    "The browser console is useful useful for inspecting the HTML structure of pages. If you look at the contents of the browse page, you'll see that the details for each issue are presented as a definition list (`<dl>`), inside a `<div>` with the class of `l-item-info`. This information tells us the paths we need to follow to get to the issue metadata.\n",
    "\n",
    "That's really all we need to know to start harvesting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just the url we found above, with a slot into which we can insert the startIdx value\n",
    "# If you want to download data from another journal, just change the nla.obj identifier to point to the journal.\n",
    "start_url = 'https://nla.gov.au/nla.obj-68375465/browse?startIdx={}&rows=20&op=c'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The initial startIdx value\n",
    "start = 0\n",
    "# Number of results per page\n",
    "n = 20\n",
    "issues = []\n",
    "# If there aren't 20 results on the page then we've reached the end, so continue harvesting until that happens.\n",
    "while n == 20:\n",
    "    # Get the browse page\n",
    "    response = requests.get(start_url.format(start))\n",
    "    # Beautifulsoup turns the HTML into an easily navigable structure\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    # Find all the divs containing issue details and loop through them\n",
    "    details = soup.find_all(class_='l-item-info')\n",
    "    for detail in details:\n",
    "        issue = {}\n",
    "        # Get the issue id\n",
    "        issue['id'] = detail.dt.a.string\n",
    "        rows = detail.find_all('dd')\n",
    "        # Get the issue details\n",
    "        issue['details'] = rows[2].p.string\n",
    "        # Get the number of pages\n",
    "        issue['pages'] = re.search(r'^(\\d+)', children, flags=re.MULTILINE).group(1)\n",
    "        issues.append(issue)\n",
    "        print(issue)\n",
    "        time.sleep(0.2)\n",
    "    # Increment the startIdx\n",
    "    start += n\n",
    "    # Set n to the number of results on the current page\n",
    "    n = len(details)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(issues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the harvested results as a JSON file in case we need them later on\n",
    "with open('{}/issues.json'.format(data_dir), 'w') as outfile:\n",
    "    json.dump(issues, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the saved JSON file\n",
    "with open('{}/issues.json'.format(data_dir), 'r') as infile:\n",
    "    issues = json.load(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up the metadata\n",
    "\n",
    "So far we've just grabbed the complete issue details as a single string. It would be good to parse this string so that we have the dates, volume and issue numbers in separate fields. As is always the case, there's a bit of variation in the way this information is recorded. The code below tries out different combinations and then saves the structured data in a Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import arrow\n",
    "from arrow.parser import ParserError\n",
    "issues_data = []\n",
    "# Loop through the issues\n",
    "for issue in issues:\n",
    "    issue_data = {}\n",
    "    issue_data['id'] = issue['id']\n",
    "    issue_data['pages'] = int(issue['pages'])\n",
    "    print(issue['details'])\n",
    "    try:\n",
    "        # This pattern looks for details in the form: Vol. 2 No. 3 (2 Jul 1878)\n",
    "        details = re.search(r'(.*)Vol. (\\d+) No\\.* (\\d+) \\((.+)\\)', issue['details'].strip())\n",
    "        issue_data['label'] = details.group(1).strip()\n",
    "        issue_data['volume'] = details.group(2)\n",
    "        issue_data['number'] = details.group(3)\n",
    "        date = details.group(4)\n",
    "    except AttributeError:\n",
    "        try:\n",
    "            # This pattern looks for details in the form: No. 3 (2 Jul 1878)\n",
    "            details = re.search(r'No. (\\d+) \\((.+)\\)', issue['details'].strip())\n",
    "            issue_data['label'] = ''\n",
    "            issue_data['volume'] = ''\n",
    "            issue_data['number'] = details.group(1)\n",
    "            date = details.group(2)\n",
    "        except AttributeError:\n",
    "            try:\n",
    "                # This pattern looks for details in the form: Bulletin Christmas Edition (2 Jul 1878)\n",
    "                details = re.search(r'(.*) \\((.+)\\)', issue['details'].strip())\n",
    "                issue_data['label'] = details.group(1)\n",
    "                issue_data['volume'] = ''\n",
    "                issue_data['number'] = ''\n",
    "                date = details.group(2)\n",
    "            except AttributeError:\n",
    "                # This pattern looks for details in the form: Bulletin 1878 Jul 3\n",
    "                details = re.search(r'Bulletin (.+)', issue['details'].strip())\n",
    "                date_str = details.group(1)\n",
    "                # Date is wrong way round, split and reverse\n",
    "                date = ' '.join(reversed(date_str.split()))\n",
    "                issue_data['label'] = ''\n",
    "                issue_data['volume'] = ''\n",
    "                issue_data['number'] = ''\n",
    "    # Normalise months\n",
    "    date = date.replace('June', 'Jun').replace('July', 'Jul').replace('Sept', 'Sep').replace('  ', ' ')\n",
    "    # Convert date to ISO format\n",
    "    issue_data['date'] = arrow.get(date, 'D MMM YYYY').isoformat()[:-15]\n",
    "    issues_data.append(issue_data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save as CSV\n",
    "\n",
    "Now the issues data is in a nice, structured form, we can load it into a Pandas dataframe. This allows us to do things like find the total number of pages digitised.\n",
    "\n",
    "We can also save the metadata as a CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Convert issues metadata into a dataframe\n",
    "df = pd.DataFrame(issues_data, columns=['id', 'label', 'volume', 'number', 'date', 'pages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the total number of pages\n",
    "df['pages'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata as a CSV.\n",
    "df.to_csv('{}/bulletin_issues.csv'.format(data_dir), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download front pages\n",
    "\n",
    "The covers of many of the digitised journals are pretty interesting. Here's some code to download images of the covers of *The Bulletin*. Unfortunately, at some point the Bulletin moved its cover artworks *inside* the journal, so the front page is mostly advertising. Of course you easily adjust this code to download a different page, or range of pages.\n",
    "\n",
    "Once again, you can find the link to download an item by opening up your browser console's network tab, and then watching what happens when you click on the 'Start download' button in Trove.\n",
    "\n",
    "You should see a url something like this:\n",
    "\n",
    "```\n",
    "https://trove.nla.gov.au/nla.obj-514230837/download?downloadOption=zip&firstPage=0&lastPage=27\n",
    "```\n",
    "\n",
    "There are for parameters we can change to control what we download and the format that it's downloaded in:\n",
    "\n",
    "* the item id (the `nla.obj` bit)\n",
    "* the `downloadOption` parameter – this can be `zip` (a zip file containing JPG images), `pdf`, or `ocr` (the OCRd text)\n",
    "* the `firstPage` parameter – what page to start from (numbering starts from 0)\n",
    "* the `lastPage` parameter – what page to stop at\n",
    "\n",
    "So to download the first page of the issue with the id of `nla.obj-514230837`, you'd use the url:\n",
    "\n",
    "```\n",
    "https://trove.nla.gov.au/nla.obj-514230837/download?downloadOption=zip&firstPage=0&lastPage=0\n",
    "```\n",
    "\n",
    "Note that the JPG and PDF files are likely to be very large, so downloading them will consume significant amounts of time and disk space.\n",
    "\n",
    "The code below checks to see if an image has already been saved before downloading it, so if the process is interrupted you can just run it again to pick up where it stopped. If more issues are added to Trove you could run it again to pick up any new images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a directory to save the images into\n",
    "output_dir = data_dir + '/images'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "# Loop through the issue metadata\n",
    "for issue in issues_data:\n",
    "    print(issue['id'])\n",
    "    id = issue['id']\n",
    "    # Check to see if the first page of this issue has already been downloaded\n",
    "    if not os.path.exists('{}/{}-1.jpg'.format(output_dir, id)):\n",
    "        url = 'https://nla.gov.au/{}/download?downloadOption=zip&firstPage=0&lastPage=0'.format(id)\n",
    "        # Get the file\n",
    "        r = requests.get(url)\n",
    "        # The image is in a zip, so we need to extract the contents into the output directory\n",
    "        z = zipfile.ZipFile(io.BytesIO(r.content))\n",
    "        z.extractall(output_dir)\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download texts\n",
    "\n",
    "As noted above, you can downloaded the OCRd text of an issue using exactly the same method. Just change the `downloadOption` to `ocr` and the `lastPage` to the number of pages in the issue minus one (because the numbering starts at zero).\n",
    "\n",
    "Some issues do not have any OCRd text and the download link returns an empty file. The code below downloads and saves all non-empty files, and stores the ids of empty files in the `empty` list for further checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a directory to save the texts into\n",
    "output_dir = data_dir + '/text'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "empty = []\n",
    "# Loop through the issues\n",
    "for issue in issues_data:\n",
    "    print(issue['id'])\n",
    "    id = issue['id']\n",
    "    # The index value for the last page of an issue will be the total pages - 1\n",
    "    last_page = int(issue['pages']) - 1\n",
    "    # Put the date in the file name for easy sorting and browsing\n",
    "    filename = '{}/{}-{}.txt'.format(output_dir, issue['date'], id)\n",
    "    # Check to see if the file has already been harvested\n",
    "    if os.path.exists(filename) and os.path.getsize(filename) > 0:\n",
    "        print('Already saved')\n",
    "    else:\n",
    "        url = 'https://trove.nla.gov.au/{}/download?downloadOption=ocr&firstPage=0&lastPage={}'.format(id, last_page)\n",
    "        # Get the file\n",
    "        r = requests.get(url)\n",
    "        # Check there was no error\n",
    "        if r.status_code == requests.codes.ok:\n",
    "            # Check that the file's not empty\n",
    "            if len(r.content) > 0:\n",
    "                # If everything's ok, save the file\n",
    "                with open(filename, 'wb') as text_file:\n",
    "                    text_file.write(r.content)\n",
    "                print('Saved')\n",
    "            else:\n",
    "                print('Empty')\n",
    "                # Store details of empty files for later\n",
    "                empty.append(id)\n",
    "            time.sleep(1)\n",
    "        else:\n",
    "            print('There was a problem: {}'.format(r.status_code))\n",
    "print(empty)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save details of empty files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the list of empty ids into a dataframe\n",
    "df_empty = pd.DataFrame(empty, columns=['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge empty ids with details dataframe to add the full details\n",
    "empty_data = pd.merge(df_empty, df, on='id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_data.to_csv('{}/bulletin_issues_empty.csv'.format(data_dir), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report on the harvest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "total_texts = len([f for f in os.listdir(data_dir + '/text') if f[-4:] == '.txt'])\n",
    "print('Report on harvest completed on {}: \\n'.format(datetime.datetime.now().strftime('%d %b %Y')))\n",
    "print('* metadata harvested for {} issues'.format(len(issues_data)))\n",
    "print('* ocr text harvested for {} issues'.format(total_texts))\n",
    "print('* {} issues contained no text'.format(len(empty)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
